import os
import sys
import pandas as pd
import logging
import pandas as pd
from pymongo import MongoClient
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from src.etl.extract import extract_all
import warnings

# ----------------------------------------------------
# ‚öôÔ∏è CONFIGURATION
# ----------------------------------------------------
warnings.filterwarnings("ignore", category=FutureWarning)

logger = logging.getLogger(__name__)
logger.info("=== [√âTAPE : TRANSFORMATION] ===")


def load_bronze_from_mongodb():
    logger.info("üöÄ Chargement des donn√©es brutes depuis MongoDB (couche Bronze).")
    client = MongoClient(os.getenv("MONGO_URI"))
    db = client[os.getenv("MONGO_DB_NAME")]

    data = {}

    for name in db.list_collection_names():
        if name.startswith("bronze_"):
            table = name.replace("bronze_", "")
            records = list(db[name].find({}, {"_id": 0, "data": 1}))
            data[table] = pd.DataFrame([r["data"] for r in records])

    return data

# ----------------------------------------------------
# üß© FONCTION PRINCIPALE
# ----------------------------------------------------
def transform_all(data=None):
    """
    √âtape de transformation l√©g√®re pour cr√©er la couche silver du pipeline :
    - Nettoyage et normalisation des avis
    - Enrichissements simples : image, abonnement, achat, m√©triques basiques
    - Export CSV propre

    Returns:
    Tuple[pd.DataFrame, pd.DataFrame]: (reviews_cleaned, rejects)
    """

    logger.info("üöÄ D√©marrage de l'√©tape de transformation.")

    # On r√©cup√®re les donn√©es brutes depuis MongoDB
    data = load_bronze_from_mongodb()

    reviews = data.get("review")
    if reviews is None:
        raise SystemExit("‚ùå Table 'review' absente des donn√©es extraites.")
    
    rejects = pd.DataFrame()

    # --- Nettoyage du texte ---
    if "r_desc" in reviews.columns:
        reviews = reviews.rename(columns={"r_desc": "review_text"})


    # --- Rejet des avis manquants ou vides (avant nettoyage) --- 
    empty_raw = reviews[
        reviews["review_text"].isna() |
        (reviews["review_text"].astype(str).str.strip() == "")
    ]

    if not empty_raw.empty:
        empty_raw = empty_raw.copy()
        empty_raw["reject_reason"] = "empty_review_raw"
        rejects = pd.concat([rejects, empty_raw], ignore_index=True)

    # --- On retire ces lignes avant la suite --- 
    reviews = reviews[
        ~(reviews["review_text"].isna() | (reviews["review_text"].astype(str).str.strip() == ""))
    ]


    # --- Nettoyage lexical ---
    reviews["review_text"] = (
        reviews["review_text"]
        .astype(str)
        .str.replace(r"<[^>]+>", "", regex=True)
        .str.replace(r"[^a-zA-Z0-9√Ä-√ø\s.,!?']", " ", regex=True)
        .str.replace(r"\s+", " ", regex=True)
        .str.strip()
        .str.lower()
    )

    # --- Rejet des avis devenus vides apr√®s nettoyage ---
    empty_clean = reviews[reviews["review_text"] == ""]

    if not empty_clean.empty:
        empty_clean = empty_clean.copy()
        empty_clean["reject_reason"] = "empty_after_cleaning"
        rejects = pd.concat([rejects, empty_clean], ignore_index=True)

    # --- On retire les avis nettoy√©s mais vides ---
    reviews = reviews[reviews["review_text"] != ""]

    # --- Normalisation du rating ---
    if "rating" in reviews.columns:
        invalid_rating = reviews[
            reviews["rating"].isna() | ~reviews["rating"].between(1, 5)
        ].copy()

        if not invalid_rating.empty:
            invalid_rating["reject_reason"] = "invalid_rating"
            rejects = pd.concat([rejects, invalid_rating], ignore_index=True)

        reviews = reviews[reviews["rating"].between(1, 5)]


     # --- Ajout de l'identifiant produit ---
    if "product_reviews" in data and {"review_id", "p_id"}.issubset(data["product_reviews"].columns):
        prod_map = (
            data["product_reviews"][["review_id", "p_id"]]
            .drop_duplicates()
            .rename(columns={"p_id": "product_id"})
        )
        reviews = reviews.merge(prod_map, on="review_id", how="left")
        logger.info("üîó Colonne 'product_id' ajout√©e √† partir de la table 'product_reviews' (renomm√©e depuis p_id).")
    else:
        logger.warning("‚ö†Ô∏è Table 'product_reviews' manquante ou colonnes absentes ‚Äî 'product_id' non ajout√©.")


    # --- Flag image ---
    if "review_images" in data:
        imgs = data["review_images"][["review_id"]].drop_duplicates()
        imgs["has_image"] = True
        reviews = reviews.merge(imgs, on="review_id", how="left")
        reviews["has_image"] = reviews["has_image"].fillna(False)

    # --- Flag abonnement actif ---
    if "subscription" in data:
        subs = data["subscription"].copy()
        subs["end_date"] = pd.to_datetime(subs["end_date"], errors="coerce")
        today = pd.Timestamp.now().normalize()
        subs["has_subscription"] = subs["end_date"].isna() | (subs["end_date"] > today)
        subs = subs.rename(columns={"c_id": "buyer_id"})
        reviews = reviews.merge(subs[["buyer_id", "has_subscription"]], on="buyer_id", how="left")
        reviews["has_subscription"] = reviews["has_subscription"].fillna(False)

    # --- Flag acheteur v√©rifi√© ---
    if "orders" in data and "buyer_id" in data["orders"].columns:
        buyers = data["orders"][["buyer_id"]].drop_duplicates()
        buyers["verified_buyer"] = True
        reviews = reviews.merge(buyers, on="buyer_id", how="left")
        reviews["verified_buyer"] = reviews["verified_buyer"].fillna(False)


    # --- S√©lection explicite des colonnes utiles ---
    expected_cols = [
        "review_id",
        "buyer_id",
        "product_id",
        "title",
        "review_text",
        "rating",
        "has_image",
        "has_subscription",
        "verified_buyer"
    ]

    existing_cols = [c for c in expected_cols if c in reviews.columns]
    missing_cols = [c for c in expected_cols if c not in reviews.columns]
    reviews = reviews[existing_cols]
    if missing_cols:
        logger.warning(f"‚ö†Ô∏è Colonnes absentes du DataFrame final : {missing_cols}")
    else:
        logger.info(f"‚úÖ Colonnes finales conformes : {existing_cols}")


    # --- Export ---
    run_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = Path("/shared") / f"reviews_cleaned_{run_ts}.csv"
    reviews.to_csv(output_file, index=False)
    logger.info(f"‚úÖ Donn√©es transform√©es sauvegard√©es : {output_file}")
    logger.info(f"üìä Dataset Silver final : {len(reviews)} lignes")
    print(f"‚úÖ {len(reviews)} avis nettoy√©s et enrichis. Voir logs/etl_pipeline.log pour le d√©tail.")

    if not rejects.empty:
        logger.warning(f"‚ö†Ô∏è {len(rejects)} lignes rejet√©es d√©tect√©es.")
        logger.info(f"Rejets par type : {rejects['reject_reason'].value_counts().to_dict()}")
        reject_file = Path("/shared") / f"reviews_rejects_{run_ts}.csv"
        rejects.to_csv(reject_file, index=False)
        logger.warning(f"‚úÖ Rejets sauvegard√©s localement : {reject_file}")
    else:
        logger.info("‚úÖ Aucune ligne rejet√©e.")
        

