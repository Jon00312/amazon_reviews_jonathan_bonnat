from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime

from src.etl.extract import extract_all
from src.etl.load_mongodb import load_to_mongodb
from src.etl.transform import transform_all
from src.etl.load import load_all
from src.nlp.zero_shot_and_scoring_job import run_zero_shot_and_scoring_job
from src.nlp.load_gold import load_gold


default_args = {
    "owner": "airflow",
    "retries": 1,
}

with DAG(
    dag_id="amazon_reviews_pipeline",
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
    default_args=default_args,
    tags=["etl", "nlp", "amazon"],
) as dag:

    extract = PythonOperator(
        task_id="extract_postgres",
        python_callable=extract_all
    )

    bronze = PythonOperator(
        task_id="load_bronze_mongo",
        python_callable=load_to_mongodb
    )

    transform = PythonOperator(
        task_id="transform_reviews",
        python_callable=transform_all
    )

    silver = PythonOperator(
        task_id="load_silver_s3",
        python_callable=load_all
    )

    nlp_job = BashOperator(
        task_id="run_zero_shot_and_scoring_job",
        bash_command="python /opt/airflow/project/src/nlp/zero_shot_and_scoring_job.py",
    )

    gold = PythonOperator(
        task_id="load_gold_s3",
        python_callable=load_gold
    )

    extract >> bronze >> transform >> silver >> nlp_job >> gold
